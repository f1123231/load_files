"""1.对目标变量进行标准化
2.对验证集和测试集也要应用与训练集相同的数据清洗策略
"""
import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
import torch.nn.functional as F
import math
import copy
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
import os


# 常量定义
DATASET_PATH = "D:\\桌面\\new2_1s_Center_freqs_and_vibration_level_bands_result.csv"
VAL_SIZE = 0.1
RANDOM_STATE = 43
LOW_FREQ_THRESHOLD = 6
BATCH_SIZE = 32
LOW_FREQ_BOOST = 2.0
LOW_FREQ_LR = 0.001
HIGH_FREQ_LR = 0.0005
ADAPTIVE_LR = 0.01
WEIGHT_DECAY = 1e-5
LR_FACTOR = 0.5
LEARNING_RATE = 0.0005
LR_PATIENCE = 10
NUM_EPOCHS = 300
PATIENCE = 15
L2_LAMBDA = 1e-4

# --- 中心频率 ---
# 10-8000Hz, 30 个频带
freq_bands = [
    10, 12.5, 16, 20, 25, 31.5, 40, 50, 63, 80, 100, 125, 160, 200, 250,
    315, 400, 500, 630, 800, 1000, 1250, 1600, 2000, 2500, 3150, 4000,
    5000, 6300, 8000
]

# 确保 freq_bands 的长度与模型输出维度 (num_tasks) 匹配
num_bands = len(freq_bands)

# --- Matplotlib 全局设置 ---
plt.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体为黑体
plt.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题


# 1.数据加载与预处理模块
def load_data(dataset_path):
    """
    加载数据集
    :param dataset_path:数据集文件的路径
    :return  dataset_path 路径下的 CSV 文件的DataFrame
    """
    return pd.read_csv(dataset_path)


def preprocess_data(data, test_condition):
    """
    预处理数据，包括数据清洗和划分数据集，注意防止数据泄露，为留一工况交叉验证生成预处理后的数据划分。
    每次yield一组完整的、经过清洗、标准化处理的训练、验证、测试张量及标准化器。
    """
    print("数据预处理开始...")

    # 创建一个新的DataFrame，只保留需要的列
    new_data = pd.concat([data.iloc[:, 1:3], data.iloc[:, 3:]], axis=1)
    data = new_data

    print(f"只保留需要的列后的data的形状：{data.shape}")

    # ①获取所有独特的工况 (speed, load) 对
    unique_conditions = data[['speed', 'load']].drop_duplicates().values.tolist()
    unique_conditions = [tuple(condition) for condition in unique_conditions]  # 转换为元组列表，方便比较

    if len(unique_conditions) < 3:
        print("警告：数据中的独立工况少于3个，无法同时划分训练、验证和测试集。")
        return  # 提前退出

    print(f"总共发现 {len(unique_conditions)} 个独立工况: {unique_conditions}")

    # ②外层循环：依次选择一个工况作为测试集
    for test_condition in unique_conditions:
        print(f"\n--- 外层循环：指定测试工况为 {test_condition} ---")

        # 分离测试集
        test_mask = (data['speed'] == test_condition[0]) & (data['load'] == test_condition[1])
        # 使用.copy()避免后续操作影响原始data或其它切片
        test_data_split = data[test_mask].copy()
        train_val_data = data[~test_mask].copy()  # 剩余数据作为训练+验证候选集

        print(f"测试工况 {test_condition} 的样本数: {len(test_data_split)}")
        print(f"用于划分训练+验证的剩余工况样本总数: {len(train_val_data)}")

        # ③获取剩余数据中的独立工况
        train_val_conditions = train_val_data[['speed', 'load']].drop_duplicates().values.tolist()
        train_val_conditions = [tuple(condition) for condition in train_val_conditions]

        if not train_val_conditions:  # 如果只有一个工况，则无法进行内层划分
            print(f"警告：移除测试工况 {test_condition} 后没有剩余工况可用于训练/验证。跳过此外层循环。")
            continue

        # ④内层循环：在剩余工况中，依次选择一个工况作为验证集
        for val_condition in train_val_conditions:
            print(f"  --- 内层循环：指定验证工况为 {val_condition} ---")

            # 从 train_val_data 中分离验证集
            val_mask = (train_val_data['speed'] == val_condition[0]) & (train_val_data['load'] == val_condition[1])

            # 使用.copy()避免后续操作影响原始data或其它切片
            val_data_split = train_val_data[val_mask].copy()

            # train_val_data 中排除验证集后即为训练集
            train_data_split = train_val_data[~val_mask].copy()

            print(f"验证工况 {val_condition} 的样本数: {len(val_data_split)}")
            print(f"训练集包含的工况数: {len(train_val_conditions) - 1}, 样本数: {len(train_data_split)}")

            # === 重构：定义数据清洗辅助函数 ===
            def clean_data_split(df_split, split_name, target_columns):
                print(f"对当前{split_name}集进行数据清洗...")
                initial_rows = len(df_split)
                if initial_rows == 0:
                    print(f"{split_name}集为空，跳过清洗")
                    return df_split  # 返回空DataFrame

                # a. 缺失值处理
                df_cleaned = df_split.dropna()
                rows_dropped = initial_rows - len(df_cleaned)
                if rows_dropped > 0:
                    print(f"  {split_name} 集缺失值处理：移除了 {rows_dropped} 行。")
                print(f"  {split_name} 数据形状(缺失值处理后): {df_cleaned.shape}")

                if df_cleaned.empty:
                    print(f"  警告：{split_name} 集在缺失值处理后为空。")
                    return df_cleaned  # 返回空 DataFrame

                # b.异常值处理（按工况分组）
                # 确保目标列存在
                valid_target_cols = [col for col in target_columns if col in df_cleaned.columns]
                if not valid_target_cols:
                    print(f"警告：{split_name} 集清洗后的数据中无有效目标列，跳过异常值处理。")
                    return df_cleaned

                if len(df_cleaned.columns) <= 2:  # 至少需要speed, load和一个目标列
                    print(f"  警告：{split_name} 集清洗后的数据列数不足，跳过异常值处理。")
                    return df_cleaned

                # 内部函数，因为需要target_columns
                def apply_process_outliers(group):
                    # 注意：这里仍然使用组内的分位数和中位数。
                    # 更严格的做法是使用训练集的统计量，但这会增加复杂性。
                    # 当前方法应用了“相同的策略”，但可能引入微小的信息泄露。
                    group = group.copy()
                    for col in valid_target_cols:
                        Q1 = group[col].quantile(0.25)
                        Q3 = group[col].quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        median = group[col].median()
                        if pd.notna(median):
                            group[col] = group[col].where((group[col] >= lower_bound) & (group[col] <= upper_bound),
                                                          median)
                        else:
                            # 如果中位数是NaN，则不替换
                            pass  # 可以选择打印警告
                        return group

                # 只有在分组键存在时才应用 groupby
                group_keys = ["speed", "load"]
                if all(key in df_cleaned.columns for key in group_keys):
                    # 检查是否有多个组，如果只有一个组，apply 可能行为不同或无意义
                    if df_cleaned[group_keys].drop_duplicates().shape[0] > 1:
                        df_cleaned = df_cleaned.groupby(group_keys, group_keys=False).apply(apply_process_outliers)
                    else:  # 如果只有一个工况组，直接在整个 DataFrame 上应用（如果需要的话）
                        df_cleaned = apply_process_outliers(df_cleaned)  # 或者决定不处理单组异常值
                    print(f"  {split_name} 数据形状(异常值处理后)：{df_cleaned.shape}")
                else:
                    print(f"  警告：{split_name} 数据中缺少 'speed' 或 'load' 列，跳过按工况分组的异常值处理。")

                if df_cleaned.empty:
                    print(f"警告：{split_name} 集在异常值处理后为空。")

                return df_cleaned

            # === 清洗函数定义结束 ===

            # ⑤对当前所有数据集应用清洗策略
            # 获取目标列名
            target_cols = data.columns[2:].tolist()  # 从原始 data 获取列名

            train_data_cleaned = clean_data_split(train_data_split, "训练", target_cols)
            val_data_cleaned = clean_data_split(val_data_split, "验证", target_cols)
            test_data_cleaned = clean_data_split(test_data_split, "测试", target_cols)

            # 检查清洗后是否有集合为空，如果关键集合为空则跳过此折
            if train_data_cleaned.empty or val_data_cleaned.empty:  # 测试集为空可能仍能接受，但训练/验证集空则无法继续
                print(
                    f"  警告：当前划分下训练集或验证集在清洗后为空 (测试: {test_condition}, 验证: {val_condition})。跳过此组合。")
                continue

            # ⑥提取特征 (X) 和标签 (y)，从 清洗后的数据集中提取
            try:
                X_train_cleaned = train_data_cleaned.iloc[:, :2].values
                y_train_cleaned = train_data_cleaned.iloc[:, 2:].values

                X_val_cleaned = val_data_cleaned.iloc[:, :2].values
                y_val_cleaned = val_data_cleaned.iloc[:, 2:].values

                # 测试集可能在清洗后变为空，需要处理这种情况
                if not test_data_cleaned.empty:
                    X_test_cleaned = test_data_cleaned.iloc[:, :2].values
                    y_test_cleaned = test_data_cleaned.iloc[:, 2:].values
                else:
                    # 如果测试集为空，创建空的 numpy 数组以避免后续错误，或者决定跳过此折
                    print(f"  警告：测试集 (工况 {test_condition}) 在清洗后为空。")
                    X_test_cleaned = np.empty((0, X_train_cleaned.shape[1]), dtype=np.float32)
                    y_test_cleaned = np.empty((0, y_train_cleaned.shape[1]), dtype=np.float32)
                    # 如果测试集为空则评估无意义，可以选择 continue 跳过此折
                    # continue

            except (KeyError, IndexError) as e:  # 捕获可能的索引错误
                print(f"  错误：提取 X, y 时出错: {e}。请检查清洗后的数据列。跳过此组合。")
                continue

            # ⑦ 标准化特征 (X)  (使用清洗后的 X_train_cleaned, X_val_cleaned, X_test_cleaned)
            x_scaler = StandardScaler()
            X_train_scaled = x_scaler.fit_transform(X_train_cleaned)
            X_val_scaled = x_scaler.transform(X_val_cleaned)
            # 只有在测试集非空时才转换
            if X_test_cleaned.shape[0] > 0:
                X_test_scaled = x_scaler.transform(X_test_cleaned)
            else:
                X_test_scaled = np.empty((0, X_train_scaled.shape[1]), dtype=np.float32)

            # ⑦ 标准化目标变量 (y)  (使用清洗后的 y_train_cleaned, y_val_cleaned, y_test_cleaned)
            y_scaler = StandardScaler()
            y_train_scaled = y_scaler.fit_transform(y_train_cleaned)
            y_val_scaled = y_scaler.transform(y_val_cleaned)
            # 只有在测试集非空时才转换
            if y_test_cleaned.shape[0] > 0:
                y_test_scaled = y_scaler.transform(y_test_cleaned)
            else:
                y_test_scaled = np.empty((0, y_train_scaled.shape[1]), dtype=np.float32)

            # ⑧ 转换为 PyTorch 张量  (使用 scaled 的 X 和 y)
            X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
            y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)
            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)
            y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)
            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
            y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)

            # ⑨ yield 当前处理好的数据划分
            yield (X_train_tensor, y_train_tensor,
                   X_val_tensor, y_val_tensor,
                   X_test_tensor, y_test_tensor,
                   x_scaler, y_scaler,
                   test_condition, val_condition)

        print("\n数据预处理生成器完成所有划分。")


def create_dataloaders(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, batch_size=BATCH_SIZE):
    """创建训练集、测试集和验证集的数据加载器."""
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader, val_loader


# 2.模型定义模块
class AttentionLayer(nn.Module):
    """注意力层."""
    def __init__(self, input_dim):
        super(AttentionLayer, self).__init__()
        self.attention_weights = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        weights = torch.softmax(self.attention_weights(x), dim=1)
        return x * weights


class FrequencyAwareProbabilisticMultiTaskModel(nn.Module):
    """频带感知概率输出多任务模型."""
    def __init__(self, input_dim, output_dim, low_freq_threshold=LOW_FREQ_THRESHOLD):
        super(FrequencyAwareProbabilisticMultiTaskModel, self).__init__()
        if low_freq_threshold >= output_dim or low_freq_threshold < 0:
            raise ValueError("low_freq_threshold must be between 0 and output_dim-1")

        self.low_freq_tasks = low_freq_threshold
        self.high_freq_tasks = output_dim - low_freq_threshold
        self.total_tasks = output_dim

        # 共享特征提取层
        self.shared_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        self.shared_attention = AttentionLayer(256)

        # 低频特化特征提取层
        self.low_freq_extractor = nn.Sequential(
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU()
        )
        self.low_freq_attention = AttentionLayer(128)

        # 高频特化特征提取层
        self.high_freq_extractor = nn.Sequential(
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU()
        )
        self.high_freq_attention = AttentionLayer(128)

        # 低频任务特定头
        self.low_freq_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(128, 64),
                nn.LayerNorm(64),
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, 2)  # 输出两个值：均值和标准差
            ) for _ in range(self.low_freq_tasks)
        ])

        # 高频任务特定头
        self.high_freq_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(128, 64),
                nn.LayerNorm(64),
                nn.ReLU(),
                nn.Linear(64, 2)
            ) for _ in range(self.high_freq_tasks)
        ])

    def forward(self, x):
        # 共享特征提取
        shared_features = self.shared_feature_extractor(x)
        # 对共享特征应用注意力
        shared_features = self.shared_attention(shared_features)

        # 低频和高频特征提取
        low_freq_features = self.low_freq_extractor(shared_features)
        high_freq_features = self.high_freq_extractor(shared_features)

        # 对低频和高频特征应用注意力
        low_freq_features = self.low_freq_attention(low_freq_features)
        high_freq_features = self.high_freq_attention(high_freq_features)

        # 处理头
        all_means = []
        all_stds = []

        # 低频输出
        for i in range(self.low_freq_tasks):
            head_output = self.low_freq_heads[i](low_freq_features)  # Shape: (batch_size, 2)
            mean = head_output[:, 0].unsqueeze(1)  # Shape: (batch_size, 1)
            std_raw = head_output[:, 1].unsqueeze(1)  # Shape: (batch_size, 1)
            # 应用 softplus 以确保 std > 0，添加 epsilon 以保持稳定性
            std = F.softplus(std_raw) + 1e-6
            all_means.append(mean)
            all_stds.append(std)

        # 高频输出
        for i in range(self.high_freq_tasks):
            head_output = self.high_freq_heads[i](high_freq_features)  # Shape: (batch_size, 2)
            mean = head_output[:, 0].unsqueeze(1)  # Shape: (batch_size, 1)
            std_raw = head_output[:, 1].unsqueeze(1)  # Shape: (batch_size, 1)
            # 应用 softplus 以确保 std > 0，添加 epsilon 以保持稳定性
            std = F.softplus(std_raw) + 1e-6
            all_means.append(mean)
            all_stds.append(std)

        # 合并输出
        means_tensor = torch.cat(all_means, dim=1)  # Shape: (batch_size, total_tasks)
        stds_tensor = torch.cat(all_stds, dim=1)  # Shape: (batch_size, total_tasks)

        return means_tensor, stds_tensor


# 3.损失函数定义模块,高斯负对数似然损失
# 此损失函数计算 NLL，假设目标 'y' 来自高斯分布，预测平均值为 'mu' 和标准差 'sigma'。
class GaussianNLLLoss(nn.Module):
    """高斯负对数似然损失"""
    def __init__(self, reduction="mean"):
        super(GaussianNLLLoss, self).__init__()
        self.reduction = reduction

    def forward(self, mu, sigma, target):
        """
        计算高斯负对数似然损失
        :param mu:预测的平均值（batch_size, num_tasks）
        :param sigma:预测的标准差（batch_size, num_tasks）
        :param target:真实值（batch_size, num_tasks）
        :return:Tensor,计算出的NLL损失
        """
        # 要确保sigma是正的
        log_sigma = torch.log(sigma)
        term1 = 0.5 * torch.log(torch.tensor(2.0 * math.pi))  # 常数项log(sqrt(2pi))
        term2 = log_sigma
        term3 = 0.5 * ((target - mu) / sigma)**2  # (y - mu)^2 / (2 * sigma^2)

        nll_elementwise = term1 + term2 + term3

        # 应用reduction
        if self.reduction == 'mean':
            return torch.mean(nll_elementwise)
        elif self.reduction == 'sum':
            return torch.sum(nll_elementwise)
        else:  # 'none'
            return nll_elementwise


class FrequencyAwareAdaptiveWeightLoss(nn.Module):
    """频带感知自适应权重损失函数.它是用来进行损失权重调整的"""
    def __init__(self, num_tasks, low_freq_boost=LOW_FREQ_BOOST, low_freq_threshold=LOW_FREQ_THRESHOLD):
        super(FrequencyAwareAdaptiveWeightLoss, self).__init__()
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))  # 可学习的对数方差参数
        self.low_freq_boost = low_freq_boost
        self.low_freq_threshold = low_freq_threshold
        self.task_weights = torch.ones(num_tasks) / num_tasks

    def forward(self, losses):
        weights = torch.exp(-self.log_vars)  # 根据对数方差计算权重
        boosted_weights = weights.clone()
        boosted_weights[:self.low_freq_threshold] *= self.low_freq_boost  # 低频任务权重增强
        self.task_weights = boosted_weights.detach()
        weighted_losses = boosted_weights * losses + 0.5 * self.log_vars  # 最终损失公式
        return weighted_losses.mean(), boosted_weights

# 4.模型训练与验证模块
def prepare_optimizer_and_loss(model, output_dim, device):
    """为频带感知概率输出多任务模型准备优化器、学习率调度器和损失函数"""
    print("准备优化器和损失函数...")

    # 1.自适应权重损失函数，这个函数接收每个任务的损失，并计算加权总损失和正则项
    adaptive_loss_func = FrequencyAwareAdaptiveWeightLoss(num_tasks=output_dim, low_freq_boost=LOW_FREQ_BOOST,
                                                          low_freq_threshold=LOW_FREQ_THRESHOLD).to(device)
    print(f"- 自适应损失函数 (FrequencyAwareAdaptiveWeightLoss) 创建成功，低频阈值: {LOW_FREQ_THRESHOLD}, 低频增强: {LOW_FREQ_BOOST}")

    # 2.主损失函数（高斯负对数似然损失）
    # 设置 reduction='none' 以便获取每个样本每个任务的损失值，这对于后续计算 per-task loss (在批次维度求平均) 并输入到 adaptive_loss_func 是必需的。
    nll_loss_fn = GaussianNLLLoss(reduction='none').to(device)
    print("  - 主损失函数 (GaussianNLLLoss, reduction='none') 创建成功。")

    # 3.定义不同部分的参数组
    try:
        low_freq_params = list(model.low_freq_extractor.parameters()) + \
                          [p for head in model.low_freq_heads for p in head.parameters()]  # 简化获取低频头参数
        high_freq_params = list(model.high_freq_extractor.parameters()) + \
                           [p for head in model.high_freq_heads for p in head.parameters()]  # 简化获取高频头参数
        shared_params = list(model.shared_feature_extractor.parameters()) + \
                        list(model.shared_attention.parameters()) + \
                        list(model.low_freq_attention.parameters()) + \
                        list(model.high_freq_attention.parameters())  # 将所有注意力层归入共享参数

        # 添加adaptive_loss_func的可学习参数（log_vars）到优化器
        adaptive_loss_params = list(adaptive_loss_func.parameters())

        print(f"  - 参数分组：共享层 {len(shared_params)} 个参数张量，"
              f"低频特定层 {len(low_freq_params)} 个，"
              f"高频特定层 {len(high_freq_params)} 个，"
              f"自适应损失 {len(adaptive_loss_params)} 个。")

    except AttributeError as e:
        print(f"错误：访问模型参数时出错: {e}。请检查模型结构是否与参数分组代码匹配。")
        raise  # 重新抛出异常，因为这是关键错误

    # 4.创建优化器（Adam），为不同参数组设置不同的学习率
    optimizer = optim.Adam([
        {'params': shared_params, 'lr': LEARNING_RATE},  # 共享层使用基础学习率
        {'params': low_freq_params, 'lr': LOW_FREQ_LR},  # 低频层使用特定学习率
        {'params': high_freq_params, 'lr': HIGH_FREQ_LR},  # 高频层使用特定学习率
        {'params': adaptive_loss_params, 'lr': ADAPTIVE_LR}  # 自适应损失参数使用特定学习率
    ], weight_decay=WEIGHT_DECAY)  # 全局应用 weight decay
    print(
        f"  - Adam 优化器创建成功。基础 LR: {LEARNING_RATE}, 低频 LR: {LOW_FREQ_LR}, 高频 LR: {HIGH_FREQ_LR}, 自适应损失 LR: {ADAPTIVE_LR}, Weight Decay: {WEIGHT_DECAY}")

    # 5.创建学习率调度器
    # 监控验证集损失，当不再改善时降低学习率
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='min',  # 监控指标越小越好 (通常是验证集损失)
        factor=LR_FACTOR,  # 学习率衰减因子
        patience=LR_PATIENCE,  # 容忍多少个 epoch 没有改善
        verbose=True  # 触发时打印信息
    )
    print(f"  - ReduceLROnPlateau 学习率调度器创建成功。Factor: {LR_FACTOR}, Patience: {LR_PATIENCE}")

    print("优化器和损失函数准备完毕。\n")
    return optimizer, scheduler, adaptive_loss_func, nll_loss_fn


def train_epoch(model, train_loader, optimizer, adaptive_loss_func, nll_loss_fn, l2_lambda, device):
    """
    训练一个epoch
    :param model:频带感知概率输出多任务模型实例
    :param train_loader:训练数据加载器
    :param optimizer:优化器实例
    :param adaptive_loss_func:频带感知自适应权重损失函数实例
    :param nll_loss_fn:高斯负对数似然损失函数实例 (reduction应为'none'或能在内部处理)
    :param l2_lambda:L2 正则化系数
    :param device:计算设备
    :return:float: 当前 epoch 的平均训练损失
    """
    model.train()  # 设置模型为训练模式
    train_running_loss = 0.0
    total_batches = len(train_loader)

    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)

        # 1.梯度清零
        optimizer.zero_grad()

        # 2.前向传播
        # 模型输出预测的均值（means）和标准差（stds）
        predicted_means, predicted_stds = model(inputs)

        # 3.计算主损失
        # 计算每个样本每个任务的NLL损失（需要nll_loss_fn的reduction="none"）
        # 确保传入的nll_loss_fn的reduction不是"mean"或"sum"
        nll_loss_elementwise = nll_loss_fn(predicted_means, predicted_stds, targets)

        # 计算每个任务的平均NLL损失（在batch维度上求平均）
        nll_loss_per_task = torch.mean(nll_loss_elementwise, dim=0)  # shape:(num_tasks,)

        # 4.应用自适应权重损失函数
        # 输入每个任务的平均NLL损失，得到加权后的总损失和任务权重
        adaptive_weighted_loss, task_weights = adaptive_loss_func(nll_loss_per_task)

        # 5.计算L2正则化项
        l2_reg = torch.tensor(0., device=device)
        # 注意力层（低正则化）
        for param in model.shared_attention.parameters():
            if param.requires_grad:
                l2_reg += 0.1 * l2_lambda * torch.norm(param, p=2)  # 使用 p=2 计算 L2 范数

        # 共享特征提取层（全量正则化）
        for param in model.shared_feature_extractor.parameters():
            if param.requires_grad:
                l2_reg += l2_lambda * torch.norm(param, p=2)

        # 低频特征提取层（低正则化）
        for param in model.low_freq_extractor.parameters():
            if param.requires_grad:
                l2_reg += 0.1 * l2_lambda * torch.norm(param, p=2)

        # 高频特征提取层（全量正则化）
        for param in model.high_freq_extractor.parameters():
            if param.requires_grad:
                l2_reg += l2_lambda * torch.norm(param, p=2)

        # 任务头部（全量正则化）
        # 低频头
        for head in model.low_freq_heads:
            for param in head.parameters():
                if param.requires_grad:
                    l2_reg += l2_lambda * torch.norm(param, p=2)
        # 高频头
        for head in model.high_freq_heads:
            for param in head.parameters():
                if param.requires_grad:
                    l2_reg += l2_lambda * torch.norm(param, p=2)

        # 6.计算总损失
        total_loss = adaptive_weighted_loss + l2_reg

        # 7.反向传播
        total_loss.backward()

        # 8.更新优化器
        optimizer.step()

        # 9.累加批次损失
        train_running_loss += total_loss.item()

    # 计算并返回epoch的平均损失
    average_epoch_loss = train_running_loss / total_batches
    return average_epoch_loss


def validate_epoch(model, val_loader, nll_loss_fn, device):
    """
    验证一个epoch
    :param model:频带感知概率输出多任务模型实例
    :param val_loader:验证数据加载器
    :param nll_loss_fn:高斯负对数似然损失函数实例 (其 reduction 参数会被忽略，内部使用 'mean')
    :param device:计算设备
    :return:- float: 当前 epoch 的平均验证 NLL 损失.
            - np.ndarray: 每个任务的平均验证 NLL 损失 (shape: [num_tasks,]).
    """
    model.eval()  # 设置模型为评估模式
    val_running_loss = 0.0
    all_nll_losses_per_task = []  # 用于存储每个批次中每个任务的平均损失
    total_batches = len(val_loader)

    with torch.no_grad():  # 在此块内禁用梯度计算
        for batch_idx, (inputs, targets) in enumerate(val_loader):
            inputs, targets = inputs.to(device), targets.to(device)

            # 1.前向传播
            predicted_means, predicted_stds = model(inputs)

            # 2.计算验证损失（使用NLL Loss）
            # a.计算元素级的 NLL 损失 (需要 nll_loss_fn 内部能处理或设置为 reduction='none')
            # 我们再次实例化一个内部使用的，确保 reduction='none'
            nll_loss_calculator_elementwise = GaussianNLLLoss(reduction='none').to(device)
            nll_loss_elementwise = nll_loss_calculator_elementwise(predicted_means, predicted_stds, targets)

            # b.计算当前批次的平均 NLL 损失 (在所有样本和所有任务上取平均)
            batch_loss = torch.mean(nll_loss_elementwise)
            val_running_loss += batch_loss.item()

            # c.计算当前批次每个任务的平均 NLL 损失 (在批次维度上求平均)
            nll_loss_per_task_batch = torch.mean(nll_loss_elementwise, dim=0)  # Shape: (num_tasks,)
            all_nll_losses_per_task.append(nll_loss_per_task_batch.cpu().numpy())

    # 计算整个验证集的平均损失
    average_val_loss = val_running_loss / total_batches

    # 计算整个验证集上每个任务的平均损失
    # 将列表中的 numpy 数组堆叠起来，然后在第一个维度（批次维度）上求平均
    avg_nll_losses_per_task = np.mean(np.array(all_nll_losses_per_task), axis=0)

    # 返回整个验证集的平均 NLL 损失 (average_val_loss) 和每个任务的平均 NLL 损失 (avg_nll_losses_per_task)。
    # average_val_loss 是用于学习率调度器 (scheduler.step()) 和模型选择（例如早停）的主要指标。avg_nll_losses_per_task 可用于分析模型在不同频带上的表现。
    return average_val_loss, avg_nll_losses_per_task


def train_model(model, train_loader, val_loader, optimizer, scheduler, adaptive_loss_func, nll_loss_func, l2_lambda, num_epochs, patience, device):
    """
    训练频带感知概率输出多任务模型
    :param model:要训练的模型实例
    :param train_loader:训练数据加载器
    :param val_loader:验证数据加载器
    :param optimizer:优化器实例
    :param scheduler:学习率调度器实例
    :param adaptive_loss_func:自适应权重损失函数实例(用于 train_epoch)
    :param nll_loss_func:高斯 NLL 损失函数实例(用于 train_epoch 和 validate_epoch)
    :param l2_lambda:L2 正则化系数 (用于 train_epoch)
    :param num_epochs:最大训练轮数
    :param patience:早停的容忍轮数 (验证损失连续多少轮未改善则停止)
    :param device:计算设备
    :return: - list[float]: 每个 epoch 的平均训练损失列表。
            - list[float]: 每个 epoch 的平均验证 NLL 损失列表。
            - float: 训练过程中达到的最佳 (最小) 验证 NLL 损失。
            - dict: 对应最佳验证损失的模型状态字典 (state_dict)。
            - np.ndarray: 每个 epoch 每个任务的平均验证 NLL 损失 (形状: [num_epochs_run, num_tasks])。
    """
    train_losses = []
    val_losses = []
    all_per_task_val_losses = []  # 存储每个epoch的每个任务的验证损失
    best_val_loss = float("inf")
    best_model_weights = None
    epochs_no_improve = 0

    print(f"开始训练模型，共{num_epochs}个epochs，早停耐心设置为{patience}...")

    for epoch in range(num_epochs):
        # --- 训练阶段 ---
        avg_train_loss = train_epoch(
            model,
            train_loader,
            optimizer,
            adaptive_loss_func,
            nll_loss_func,  # NLL loss (reduction='none')
            l2_lambda,
            device
        )
        train_losses.append(avg_train_loss)

        # --- 验证阶段 ---
        avg_val_loss, avg_val_losses_per_task = validate_epoch(
            model,
            val_loader,
            nll_loss_func,  # NLL loss (内部会用 reduction='none' 计算)
            device
        )
        val_losses.append(avg_val_loss)
        all_per_task_val_losses.append(avg_val_losses_per_task)

        print(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val NLL Loss: {avg_val_loss:.4f}")

        # --- 学习率调度 ---
        # ReduceLROnPlateau 需要传入监控的指标
        scheduler.step(avg_val_loss)

        # --- 早停逻辑 ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # 使用 copy.deepcopy 或 state_dict().copy() 来保存权重，避免引用问题
            best_model_weights = copy.deepcopy(model.state_dict())
            epochs_no_improve = 0
            print(f"  Validation loss 改善，保存当前最佳模型。")
        else:
            epochs_no_improve += 1
            print(f"  Validation loss 未改善 ({epochs_no_improve}/{patience}).")
            if epochs_no_improve >= patience:
                print(f"\n早停触发：验证损失连续 {patience} epochs 未改善。停止训练于 epoch {epoch + 1}。")
                break

    # 训练结束后加载最佳权重
    if best_model_weights:
        print(f"\n训练结束。加载验证损失最小 (NLL Loss: {best_val_loss:.4f}) 时的模型权重。")
        model.load_state_dict(best_model_weights)
    else:
        print("\n警告：训练未完成或未能找到更优的模型权重。")

    # 将收集到的每任务损失列表转换为 NumPy 数组
    final_per_task_val_losses = np.array(all_per_task_val_losses)

    return train_losses, val_losses, best_val_loss, best_model_weights, final_per_task_val_losses


# 5.模型评估模块
def evaluate_model(model, test_loader, nll_loss_func, y_scaler, device):
    """
    在测试集上评估模型
    :param model:已训练好的模型实例 (通常已加载最佳权重)
    :param test_loader:测试数据加载器
    :param nll_loss_func:高斯负对数似然损失函数实例 (其 reduction 参数会被忽略)
    :param y_scaler:用于目标变量标准化的 StandardScaler 实例
    :param device:计算设备
    :return: - float: 测试集上的平均 NLL 损失。(标准化尺度)
            - float: 测试集上基于预测均值的平均 MSE 损失。(原始尺度)
            - float: 测试集上基于预测均值的平均 MAE 损失。(原始尺度)
            - np.ndarray: 所有测试样本的真实标签值 (shape: [num_test_samples, num_tasks])。(原始尺度)
            - np.ndarray: 所有测试样本的预测均值 (shape: [num_test_samples, num_tasks])。(原始尺度)
            - np.ndarray: 所有测试样本的预测标准差 (shape: [num_test_samples, num_tasks])(近似原始尺度)
    """
    model.eval()  # 设置模型为评估模式
    total_nll_loss = 0.0
    all_true_values_scaled = []
    all_pred_means_scaled = []
    all_pred_stds_scaled = []
    total_batches = len(test_loader)

    # 用于计算NLL的实例（确保 reduction='mean' 或在内部处理）
    nll_calculator = GaussianNLLLoss(reduction='mean').to(device)

    print("开始在测试集上评估模型...")

    with torch.no_grad():  # 禁用梯度计算
        for batch_idx, (inputs, targets_scaled) in enumerate(test_loader):
            inputs, targets_scaled = inputs.to(device), targets_scaled.to(device)

            predicted_means_scaled, predicted_stds_scaled = model(inputs)

            # --- 计算 NLL 损失 (在标准化尺度上) ---
            batch_nll_loss = nll_calculator(predicted_means_scaled, predicted_stds_scaled, targets_scaled)
            total_nll_loss += batch_nll_loss.item()

            # --- 收集 scaled 结果，用于后续逆转换 ---
            all_true_values_scaled.append(targets_scaled.cpu().numpy())
            all_pred_means_scaled.append(predicted_means_scaled.cpu().numpy())
            all_pred_stds_scaled.append(predicted_stds_scaled.cpu().numpy())

    # --- 循环结束后，进行逆转换 ---
    final_true_scaled = np.vstack(all_true_values_scaled)
    final_pred_means_scaled = np.vstack(all_pred_means_scaled)
    final_pred_stds_scaled = np.vstack(all_pred_stds_scaled)

    # 逆转换为原始尺度
    final_true_values_orig = y_scaler.inverse_transform(final_true_scaled)
    final_pred_means_orig = y_scaler.inverse_transform(final_pred_means_scaled)
    # 标准差的逆转换近似：乘以 scaler 的 scale_ 属性
    # 注意：这只是一个近似，统计意义上并不完美
    final_pred_stds_orig = final_pred_stds_scaled * y_scaler.scale_

    # --- 在原始尺度上计算 MSE 和 MAE ---
    avg_mse_loss_orig = np.mean((final_pred_means_orig - final_true_values_orig) ** 2)
    avg_mae_loss_orig = np.mean(np.abs(final_pred_means_orig - final_true_values_orig))

    # --- 计算平均 NLL (标准化尺度) ---
    avg_nll_loss_scaled = total_nll_loss / total_batches

    print(f"\n测试集评估完成:")
    print(f"  平均 NLL Loss (标准化尺度): {avg_nll_loss_scaled:.4f}")
    print(f"  平均 MSE Loss (原始尺度): {avg_mse_loss_orig:.4f}")
    print(f"  平均 MAE Loss (原始尺度): {avg_mae_loss_orig:.4f}")

    # 返回 NLL(scaled), MSE(orig), MAE(orig) 和 原始尺度的值
    return avg_nll_loss_scaled, avg_mse_loss_orig, avg_mae_loss_orig, \
        final_true_values_orig, final_pred_means_orig, final_pred_stds_orig


# 6.结果可视化模块
# --- 函数 1: 绘制单折预测与不确定性 ---
def plot_prediction_uncertainty(final_true_values_orig, final_pred_means_orig, final_pred_stds_orig,
                                test_condition, freq_bands, save_dir=None):
    """
    绘制单个测试工况下，平均预测值、平均真实值以及预测不确定性。(使用原始尺度数据)
    :param final_true_values_orig: 真实值 (原始尺度, num_samples, num_bands)
    :param final_pred_means_orig: 预测均值 (原始尺度, num_samples, num_bands)
    :param final_pred_stds_orig: 预测标准差 (近似原始尺度, num_samples, num_bands)
    :param test_condition: 当前测试工况 (Speed, Load)
    :param freq_bands: 中心频率列表
    :param save_dir: 保存图像的目录. Defaults to None (不保存)
    """
    if final_true_values_orig.ndim != 2 or final_pred_means_orig.ndim != 2 or final_pred_stds_orig.ndim != 2:
        print("错误：输入数组应为二维 (samples, bands)。")
        return
    if len(freq_bands) != final_true_values_orig.shape[1]:
        print(f"错误：freq_bands 长度 ({len(freq_bands)}) 与数据维度 ({final_true_values_orig.shape[1]}) 不匹配。")
        return

    # 计算平均值和标准差
    avg_true_orig = np.mean(final_true_values_orig, axis=0)
    avg_pred_mean_orig = np.mean(final_pred_means_orig, axis=0)
    avg_pred_std_orig = np.mean(final_pred_stds_orig, axis=0)  # 平均预测标准差 (近似原始尺度)

    plt.figure(figsize=(12, 6))
    plt.plot(freq_bands, avg_true_orig, 'bo-', label='平均真实值 (Avg True- Original Scale)', markersize=4)
    plt.plot(freq_bands, avg_pred_mean_orig, 'rx-', label='平均预测均值 (Avg Pred Mean- Original Scale)', markersize=4)

    # 绘制不确定性区域 (+/- 1 个平均标准差, 近似原始尺度)
    plt.fill_between(freq_bands,
                     avg_pred_mean_orig - avg_pred_std_orig,
                     avg_pred_mean_orig + avg_pred_std_orig,
                     color='r', alpha=0.2, label='预测不确定性 (±1 Avg Pred Std- Approx. Original Scale)')

    plt.xlabel("10-8000Hz三分之一倍频程中心频率 (Hz)")
    plt.ylabel("振动加速度级 (dB)")  # 确认单位
    plt.title(f"工况 {test_condition}：预测均值 vs 真实值")
    plt.xticks(freq_bands, labels=[str(f) for f in freq_bands], rotation=45, ha='right')
    plt.grid(True, which="both", ls="--", alpha=0.5)
    plt.legend()
    plt.tight_layout()

    if save_dir:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        save_path = os.path.join(save_dir, f"prediction_uncertainty_cond_{test_condition[0]}_{test_condition[1]}.png")
        plt.savefig(save_path)
        print(f"图表已保存至: {save_path}")
    else:
        plt.show()
    plt.close()  # 关闭图形，防止在循环中累积


# --- 函数 2: 计算单折误差指标 ---
def calculate_fold_errors(final_true_values_orig, final_pred_means_orig, epsilon=1e-6):
    """
    计算单个交叉验证折的各种误差指标
    :param final_true_values_orig:真实值 (num_samples, num_bands)
    :param final_pred_means_orig:预测均值 (num_samples, num_bands)
    :param epsilon:用于避免除零的小常数. Defaults to 1e-6
    :return:- np.ndarray: 每个频带的平均有符号差值 (pred - true) (shape: [num_bands,]).
            - np.ndarray: 每个频带的平均绝对差值 |pred - true| (shape: [num_bands,]).
            - np.ndarray: 每个频带的平均绝对相对误差 |pred - true| / |true| (shape: [num_bands,])
    """
    if final_true_values_orig.shape != final_pred_means_orig.shape:
        raise ValueError("真实值和预测均值的形状必须匹配。")
    if final_true_values_orig.ndim != 2:
        raise ValueError("输入数组必须是二维 (samples, bands)。")

    # 计算差值
    diff = final_pred_means_orig - final_true_values_orig
    abs_diff = np.abs(diff)

    # 计算相对误差，处理分母为零的情况
    abs_rel_err = abs_diff / (np.abs(final_true_values_orig)+epsilon)

    # 计算每个频带的平均值
    avg_diff_band = np.mean(diff, axis=0)
    avg_abs_diff_band = np.mean(abs_diff, axis=0)
    avg_rel_err_band = np.mean(abs_rel_err, axis=0)

    return avg_diff_band, avg_abs_diff_band, avg_rel_err_band


# --- 函数 3: 分析和绘制跨折误差 ---
def analyze_and_plot_cross_fold_errors(all_errors, all_conditions, freq_bands, save_dir=None):
    """
    聚合所有交叉验证折的误差，并生成汇总图表
    :param all_errors:字典，键为工况元组 (Speed, Load)，值为 calculate_fold_errors 返回的元组。
                           e.g., {(1000, 0): (avg_diff, avg_abs_diff, avg_rel_err), ...}
    :param all_conditions:所有唯一工况元组的列表，用于确定图表顺序
    :param freq_bands:中心频率列表
    :param save_dir:保存图像的目录. Defaults to None (不保存)
    """
    if not all_errors:
        print("错误：'all_errors' 字典为空，无法进行分析。")
        return

    num_bands = len(freq_bands)
    num_conditions = len(all_conditions)

    # 准备数据存储
    max_abs_diffs = []  # 每个工况的最大绝对差值 (用于柱状图)
    avg_abs_diffs = []  # 每个工况的平均绝对差值 (用于折线图)
    diff_matrix = np.zeros((num_conditions, num_bands))  # 有符号差值热力图数据
    avg_rel_errors = []  # 每个工况的平均相对误差 (用于柱状图)
    rel_err_matrix = np.zeros((num_conditions, num_bands))  # 相对误差热力图数据

    condition_labels = [f"{c[0]},{c[1]}" for c in all_conditions]  # 用于绘图的标签

    # 聚合数据
    for i, condition in enumerate(all_conditions):
        if condition not in all_errors:
            print(f"警告：工况 {condition} 的误差数据缺失，将跳过。")
            # 可以选择填充NaN或0，这里暂时跳过
            max_abs_diffs.append(np.nan)
            avg_abs_diffs.append(np.nan)
            avg_rel_errors.append(np.nan)
            continue

        avg_diff_band, avg_abs_diff_band, avg_rel_err_band = all_errors[condition]

        # 最大绝对差值、平均绝对差值数据
        max_abs_diffs.append(np.max(avg_abs_diff_band))
        avg_abs_diffs.append(np.mean(avg_abs_diff_band))

        # 绝对差值热图数据
        diff_matrix[i, :] = avg_diff_band

        # 平均相对误差数据
        avg_rel_errors.append(np.mean(avg_rel_err_band))

        # 相对误差热图数据
        rel_err_matrix[i, :] = avg_rel_err_band

    # --- 绘图 ---
    if save_dir and not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # 图 2: 最大 & 平均绝对差值图 (柱状图 + 折线图)
    plt.figure(figsize=(max(10, num_conditions * 0.5), 6))  # 动态调整宽度
    bar_positions = np.arange(num_conditions)
    plt.bar(bar_positions, max_abs_diffs, width=0.6, label='各频带最大平均绝对差值', color='skyblue')
    plt.plot(bar_positions, avg_abs_diffs, 'ro-', label='所有频带平均绝对差值', linewidth=2, markersize=5)
    plt.xlabel("工况 (速度, 负载)")
    plt.ylabel("绝对差值 (dB)")  # 确认单位
    plt.title("各工况下的最大与平均绝对预测差值")
    plt.xticks(bar_positions, condition_labels, rotation=45, ha='right')
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    if save_dir:
        plt.savefig(os.path.join(save_dir, "max_avg_absolute_difference.png"))
    else:
        plt.show()
    plt.close()

    # 图 3: 预测偏差 (有符号差值) 热力图
    plt.figure(figsize=(15, max(5, num_conditions * 0.3)))  # 动态调整高度
    sns.heatmap(diff_matrix, xticklabels=[str(f) for f in freq_bands], yticklabels=condition_labels,
                cmap="coolwarm", annot=False, fmt=".2f", center=0)  # coolwarm: 蓝负红正, center=0使0为白色
    plt.xlabel("中心频率 (Hz)")
    plt.ylabel("工况 (速度, 负载)")
    plt.title("各工况各频带的平均预测偏差 (预测值 - 真实值)")
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    if save_dir:
        plt.savefig(os.path.join(save_dir, "signed_difference_heatmap.png"))
    else:
        plt.show()
    plt.close()

    # 图 4: 平均相对误差柱状图
    plt.figure(figsize=(max(10, num_conditions * 0.5), 6))  # 动态调整宽度
    plt.bar(bar_positions, avg_rel_errors, width=0.6, label='平均相对误差', color='lightcoral')
    plt.xlabel("工况 (速度, 负载)")
    plt.ylabel("平均相对误差")
    plt.title("各工况下的平均相对预测误差")
    plt.xticks(bar_positions, condition_labels, rotation=45, ha='right')
    # plt.legend() # 如果只有一个bar，可以不用图例
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    if save_dir:
        plt.savefig(os.path.join(save_dir, "average_relative_error_bar.png"))
    else:
        plt.show()
    plt.close()

    # 图 5: 平均相对误差热力图
    plt.figure(figsize=(15, max(5, num_conditions * 0.3)))  # 动态调整高度
    sns.heatmap(rel_err_matrix, xticklabels=[str(f) for f in freq_bands], yticklabels=condition_labels,
                cmap="viridis", annot=False, fmt=".2f")  # viridis: 常见的表示大小的热图颜色
    plt.xlabel("中心频率 (Hz)")
    plt.ylabel("工况 (速度, 负载)")
    plt.title("各工况各频带的平均相对预测误差")
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    if save_dir:
        plt.savefig(os.path.join(save_dir, "relative_error_heatmap.png"))
    else:
        plt.show()
    plt.close()

    print("跨折误差分析和绘图完成")


# 主程序入口
if __name__ == "__main__":
    # ---设备设置---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"当前使用设备：{device}")

    # ---保存路径设置---
    PLOT_SAVE_DIR = "analysis_and_plots"
    if not os.path.exists(PLOT_SAVE_DIR):
        os.makedirs(PLOT_SAVE_DIR)
        print(f"创建绘图保存目录：{PLOT_SAVE_DIR}")

    # ---1. 数据加载---
    print("开始加载数据...")
    data = load_data(DATASET_PATH)
    print(f"数据加载完成，形状：{data.shape}")

    # 获取所有唯一工况，用于后续绘图和确保顺序一致
    all_conditions_list = sorted(data[["speed", "load"]].drop_duplicates().values.tolist())
    all_conditions_list = [tuple(c) for c in all_conditions_list]  # 确保是元组列表
    print(f"发现{len(all_conditions_list)}个唯一工况：{all_conditions_list}")

    # ---2. 初始化结果存储---
    all_fold_errors_results = {}  # 用于存储每个测试工况折的误差指标
    all_fold_test_metrics = {}  # 存储每个折的测试集 NLL, MSE, MAE
    fold_counter = 0

    # ---3. 创建数据预处理生成器---
    # preprocess_data 的第二个参数 test_condition 在其内部循环中使用，这里传递 None
    data_generator = preprocess_data(data, test_condition=None)

    # ---4.交叉验证循环---
    print("\n===== 开始留一工况交叉验证 =====")
    for fold_data in data_generator:
        fold_counter += 1
        try:
            # 解包时需要对应 preprocess_data 的 yield 顺序
            (X_train_tensor, y_train_tensor,
             X_val_tensor, y_val_tensor,
             X_test_tensor, y_test_tensor,
             x_scaler, y_scaler,  # <--- 接收 y_scaler
             test_condition, val_condition) = fold_data

            print(f"\n--- 第 {fold_counter} 折 / {len(all_conditions_list)} ---")
            print(f"  测试工况: {test_condition}, 验证工况: {val_condition}")
            print(
                f"  训练集大小: {X_train_tensor.shape[0]}, 验证集大小: {X_val_tensor.shape[0]}, 测试集大小: {X_test_tensor.shape[0]}")

            # 如果测试集为空（可能因为清洗导致），跳过当前折的评估和绘图，但仍需完成循环
            if X_test_tensor.shape[0] == 0:
                print("  警告：当前折的测试集为空，跳过模型评估和结果记录。")
                continue  # 继续下一个交叉验证折

            # --- 4.1创建数据加载器 ---
            train_loader, test_loader, val_loader = create_dataloaders(
                X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, BATCH_SIZE
            )
            print("  数据加载器创建完成。")

            # --- 4.2定义模型 ---
            input_dim = X_train_tensor.shape[1]
            output_dim = y_train_tensor.shape[1]  # num_tasks
            if output_dim != len(freq_bands):
                print(f"警告：模型输出维度({output_dim})与频带数({len(freq_bands)})不匹配！")
                # 可以选择退出或继续，但后续绘图标签可能不正确
                # exit()
            model = FrequencyAwareProbabilisticMultiTaskModel(input_dim, output_dim, LOW_FREQ_THRESHOLD).to(device)
            print(
                f"  模型 FrequencyAwareProbabilisticMultiTaskModel 初始化完成。输入维度: {input_dim}, 输出维度: {output_dim}")

            # --- 4.3准备优化器和损失函数 ---
            optimizer, scheduler, adaptive_loss_func, nll_loss_func = prepare_optimizer_and_loss(
                model, output_dim, device
            )

            # --- 4.4训练模型 ---
            print("  开始训练...")
            train_losses, val_losses, best_val_loss, best_model_weights, _ = train_model(
                model, train_loader, val_loader, optimizer, scheduler,
                adaptive_loss_func, nll_loss_func, L2_LAMBDA, NUM_EPOCHS, PATIENCE, device
            )
            # 确保加载最佳权重进行评估
            if best_model_weights:
                model.load_state_dict(best_model_weights)
                print(f"  已加载最佳模型权重 (验证 NLL Loss: {best_val_loss:.4f})。")
            else:
                print("  警告：未找到最佳模型权重，将使用训练结束时的权重进行评估。")

            # --- 4.5评估模型 ---
            print("  开始评估...")
            # evaluate_model 现在需要 y_scaler 来进行逆转换并返回原始尺度的结果
            avg_nll_scaled, avg_mse_orig, avg_mae_orig, \
                final_true_values_orig, final_pred_means_orig, final_pred_stds_orig = evaluate_model(
                model, test_loader, nll_loss_func, y_scaler, device  # <-- 传递 y_scaler
            )

            # 存储当前折的评估指标
            all_fold_test_metrics[test_condition] = {
                'NLL_scaled': avg_nll_scaled,
                'MSE_orig': avg_mse_orig,
                'MAE_orig': avg_mae_orig
            }
            print(f"  测试工况 {test_condition} 评估指标已记录。")

            # --- 计算当前折的误差 ---
            # 使用 evaluate_model 返回的原始尺度值
            avg_diff_band, avg_abs_diff_band, avg_rel_err_band = calculate_fold_errors(
                final_true_values_orig, final_pred_means_orig
            )
            all_fold_errors_results[test_condition] = (avg_diff_band, avg_abs_diff_band, avg_rel_err_band)
            print(f"  测试工况 {test_condition} 误差分析结果已记录。")

            # --- 绘制当前折的预测与不确定性图 ---
            # 使用 evaluate_model 返回的原始尺度值
            plot_prediction_uncertainty(
                final_true_values_orig, final_pred_means_orig, final_pred_stds_orig,
                test_condition, freq_bands, save_dir=PLOT_SAVE_DIR
            )

        except Exception as e:
            print(f"\n！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！")
            print(
                f"错误：在处理第 {fold_counter} 折 (测试工况: {test_condition}, 验证工况: {val_condition}) 时发生异常:")
            import traceback

            traceback.print_exc()  # 打印详细的错误追溯信息
            print(f"！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n")
            # 可以选择 continue 继续下一折，或者 break 停止整个过程
            # continue

    print("\n===== 交叉验证全部完成 =====")

    # --- 交叉验证后处理 ---
    if all_fold_errors_results:
        print("\n--- 开始绘制跨折误差分析图 ---")
        analyze_and_plot_cross_fold_errors(
            all_fold_errors_results,
            all_conditions_list,  # 使用之前获取的所有工况列表
            freq_bands,
            save_dir=PLOT_SAVE_DIR
        )
    else:
        print("没有收集到有效的误差结果，无法进行跨折分析。")

    # --- 计算并打印总体平均指标 ---
    if all_fold_test_metrics:
        # 使用列表推导和条件检查来安全地计算平均值
        nll_values = [m['NLL_scaled'] for m in all_fold_test_metrics.values() if m and 'NLL_scaled' in m]
        mse_values = [m['MSE_orig'] for m in all_fold_test_metrics.values() if m and 'MSE_orig' in m]
        mae_values = [m['MAE_orig'] for m in all_fold_test_metrics.values() if m and 'MAE_orig' in m]

        avg_nll_all = np.mean(nll_values) if nll_values else float('nan')
        avg_mse_all = np.mean(mse_values) if mse_values else float('nan')
        avg_mae_all = np.mean(mae_values) if mae_values else float('nan')

        print("\n--- 交叉验证总体平均评估指标 ---")
        print(f"  平均 NLL Loss (标准化尺度): {avg_nll_all:.4f}")
        print(f"  平均 MSE Loss (原始尺度): {avg_mse_all:.4f}")
        print(f"  平均 MAE Loss (原始尺度): {avg_mae_all:.4f}")
    else:
        print("没有收集到有效的评估指标，无法计算总体平均值。")

    print("\n脚本执行完毕。")